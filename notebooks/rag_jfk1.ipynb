{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG On JFK Speeches: Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "--------------\n",
    "In this post I venture into building a Retrival Augumented Generation (RAG) application that has been \"trained\" on President John F. Kennedy speeches. In past posts I covered how I [collected JFK speeches](http://michael-harmon.com/blog/jfk1.html) and [built a \"speech writer\"](http://michael-harmon.com/blog/jfk2.html) using a [Gated Recurrent Unit (GRU) Neural Network](https://en.wikipedia.org/wiki/Gated_recurrent_unit). In this post I improve upon on the prior work to build a RAG pipeline. \n",
    "\n",
    "The first thing I will cover is how I collected the data to include extra metadata on speeches as well as using the [Asyncio](https://docs.python.org/3/library/asyncio.html) package to reduce run time when writing to object storage. Next, I will go over how to load the json files from [Google Cloud Storage](https://cloud.google.com/storage?hl=en) using different [LangChain](https://www.langchain.com/) loaders. After that I cover how to embed documents and ingest the data into a [Pinecone Vector Database](https://pinecone.io/). In a follow up post I'll cover how to create and deploy the actual RAG application.\n",
    "\n",
    "Now I'll import all the classes and functions I will need for the rest of the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeharmon/miniconda3/envs/llm_env/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LangChain\n",
    "from langchain_google_community.gcs_file import GCSFileLoader\n",
    "from langchain_google_community.gcs_directory import GCSDirectoryLoader\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
    "\n",
    "# Google Cloud\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "credentials = service_account.Credentials.from_service_account_file('../credentials.json')\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../credentials.json\"\n",
    "\n",
    "\n",
    "# Pinecone VectorDB\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# API Keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping JFK Speeches using Asyncio\n",
    "-------------\n",
    "In the [first post](http://michael-harmon.com/blog/jfk1.html) of my work on a speecher writer I covered how to injest the JFK speeches from his [presidential library](https://www.jfklibrary.org/archives/other-resources/john-f-kennedy-speeches) into [Google Cloud Storage](https://cloud.google.com/storage?hl=en). I was never completely satisfied with the way I wrote the job before and  decided to go back and redo it using the [Asyncio](https://docs.python.org/3/library/asyncio.html) library to perform Asynchronous reading of HTML and writing json to Google cloud storage. The json documents include the text of the speech, its title, source and url for the speech. I don't want to go into the details this work, but I will say it was not as hard as I would have thought! The main thing was to turn functions which use the request package into [coroutines](https://docs.python.org/3/library/asyncio-task.html#coroutines). Informally, when using `requests.get` method to scrape the scrape a website, query a REST API or other I/O methods the process is \"blocking\". This means the Python task is not able to proceed until its receives the return value (or hears back) from the API or website. In the time the program is waiting, the threads and CPU could be doing other work. The [Asyncio](https://docs.python.org/3/library/asyncio.html) library allows Python to to free up these idling threads to do other work while waiting for I/O work to complete.\n",
    "\n",
    "If you are interested in reading more about it the script is [here](https://github.com/mdh266/rag-jfk/blob/main/scripts/extract.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading and Embedding Speeches\n",
    "\n",
    "At this point I have run the [extract.py](https://github.com/mdh266/rag-jfk/blob/main/scripts/extract.py) script which scraped the JFK libary website and converted the speeches into json. The speeches exist as json documents in [Google Cloud Storage](https://cloud.google.com/storage?hl=en) and in order to ingest it into [Pinecone](https://pinecone.io/) requires the use of the [JSONLoader](https://python.langchain.com/docs/integrations/document_loaders/json/) function from [LangChain](https://www.langchain.com/). In addition to loading the documents I also wanted to add metadata to the documents. I did so using LangChain by creating the `metadata_func` below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def metadata_func(record: Dict[str, str], metadata: Dict[str, str]) -> Dict[str, str]:\n",
    "    metadata[\"title\"] = record.get(\"title\")\n",
    "    metadata[\"source\"] = record.get(\"source\")\n",
    "    metadata[\"url\"] = record.get(\"url\")\n",
    "    metadata[\"filename\"] = record.get(\"filename\")\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put this function to use by instantiating the object and passing it as the `metadata_func` parameter,\n",
    "\n",
    "    loader = JSONLoader(\n",
    "                file_path, \n",
    "                jq_schema=jq_schema, \n",
    "                text_content=False,\n",
    "                content_key=\"text\",\n",
    "                metadata_func=metadata_func\n",
    "    )\n",
    "                \n",
    "However, I would only be able to use the `loader` object on local json document with a path (`file_path`) on my file system.\n",
    "\n",
    "In order to use this function to load json from a GCP bucket I need to create a function that takes in a file and its path (`file_path`) as well as the function to process the metadata about the speech's name, where it came from and return an instantiated `JSONLoader` object to read the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def load_json(file_path: str, jq_schema: str=\".\"):\n",
    "    return JSONLoader(\n",
    "                file_path, \n",
    "                jq_schema=jq_schema, \n",
    "                text_content=False,\n",
    "                content_key=\"text\",\n",
    "                metadata_func=metadata_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can pass this function to the LangChain's [GCFSFileLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.gcs_file.GCSFileLoader.html). I can then instantiate the class to load file the first debate between Kennedy and Nixon from my GCP bucket. The full path for this json document is,\n",
    "\n",
    "    gs://kennedyskis/1st-nixon-kennedy-debate-19600926.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to load the json document is,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GCSFileLoader(project_name=credentials.project_id,\n",
    "                       bucket=\"kennedyskis\",\n",
    "                       blob=\"1st-nixon-kennedy-debate-19600926.json\",\n",
    "                       loader_func=load_json)\n",
    "\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return a list of [LangChain Document(s)](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html). The text of the debate can be seen using the `.page_content` attribute,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Text, format, and style are as published in Freedom of Communications: Final Report of the Committee on Commerce, United States Senate..., Part III: The Joint Appearances of Senator John F. Kennedy and Vice President Richard M. Nixon and Other 1960 Campaign Presentations. 87th Congress, 1st Session, Senate Report No. 994, Part 3. Washington: U.S. Government Printing Office, 1961.]\n",
      "Monday, September 26, 1960\n",
      "Originating CBS, Chicago, Ill., All Networks carried.\n",
      "Moderator, Howard K. Smith.\n",
      "MR. SMITH: Good evening.\n",
      "The television and radio stations of the United States and their affiliated stations are proud to provide facilities for a discussion of issues in the current political campaign by the two major candidates for the presidency.\n",
      "The candidates need no introduction. The Republican candidate, Vice President Richard M. Nixon, and the Democratic candidate, Senator John F. Kennedy.\n",
      "According to rules set by the candidates themselves, each man shall make an opening statement of approx\n"
     ]
    }
   ],
   "source": [
    "print(document[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata for the document can be seen from the `.metadata` attribute,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'gs://kennedyskis/1st-nixon-kennedy-debate-19600926.json',\n",
       " 'seq_num': 1,\n",
       " 'title': 'Senator John F. Kennedy and Vice President Richard M. Nixon First Joint Radio-Television Broadcast, September 26, 1960',\n",
       " 'url': 'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/1st-nixon-kennedy-debate-19600926',\n",
       " 'filename': '1st-nixon-kennedy-debate-19600926'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This debate document (and documents in generally) usually are too long to fit in the context window of an LLM so we need to break them up into smaller pieces of texts. This process is called \"chunking\". Below I will show how to break up the Nixon-Kennedy debate into \"chunks\" of 200 characters with 20 characters that overlap between chunks. I do this using the [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) class as shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  429\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(document)\n",
    "\n",
    "print(\"Number of documents: \", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the documents and their associated metadata,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0:  [Text, format, and style are as published in Freedom of Communications: Final Report of the Committee on Commerce, United States Senate..., Part III: The Joint Appearances of Senator John F. Kennedy \n",
      " \tMetadata: {'source': 'gs://kennedyskis/1st-nixon-kennedy-debate-19600926.json', 'seq_num': 1, 'title': 'Senator John F. Kennedy and Vice President Richard M. Nixon First Joint Radio-Television Broadcast, September 26, 1960', 'url': 'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/1st-nixon-kennedy-debate-19600926', 'filename': '1st-nixon-kennedy-debate-19600926'} \n",
      "\n",
      "Doc 1:  John F. Kennedy and Vice President Richard M. Nixon and Other 1960 Campaign Presentations. 87th Congress, 1st Session, Senate Report No. 994, Part 3. Washington: U.S. Government Printing Office, \n",
      " \tMetadata: {'source': 'gs://kennedyskis/1st-nixon-kennedy-debate-19600926.json', 'seq_num': 1, 'title': 'Senator John F. Kennedy and Vice President Richard M. Nixon First Joint Radio-Television Broadcast, September 26, 1960', 'url': 'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/1st-nixon-kennedy-debate-19600926', 'filename': '1st-nixon-kennedy-debate-19600926'} \n",
      "\n",
      "Doc 2:  Printing Office, 1961.] \n",
      " \tMetadata: {'source': 'gs://kennedyskis/1st-nixon-kennedy-debate-19600926.json', 'seq_num': 1, 'title': 'Senator John F. Kennedy and Vice President Richard M. Nixon First Joint Radio-Television Broadcast, September 26, 1960', 'url': 'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/1st-nixon-kennedy-debate-19600926', 'filename': '1st-nixon-kennedy-debate-19600926'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(documents[:3]):\n",
    "    print(f\"Doc {n}: \", doc.page_content, \"\\n\", \"\\tMetadata:\", doc.metadata, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the metadata is the same for each of the documents since they all come from the same original json file. \n",
    "\n",
    "Now that we have data that is loaded, well go over how to use [embeddings](https://platform.openai.com/docs/guides/embeddings) to convert the text into vectors. I have covered embeddings in [prior posts](http://michael-harmon.com/blog/jfk2.html), so I won't go over it in much detail here. Instead I will focus on the LangChain commands needed to use embeddings. We can instantiate the LangChain [OpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/openai/) class, which uses [OpenAI's embeddings](https://openai.com), and then use the [embedd_query](https://python.langchain.com/docs/integrations/text_embedding/openai/#direct-usage) method to embed a single document as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "query = embedding.embed_query(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the first 5 entries of the vector,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries in embedded document: [-0.012023020535707474, 0.0033119581639766693, -0.005604343023151159, -0.03061368130147457, 0.013492794707417488]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 entries in embedded document:\", query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the size of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 1536\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector size:\", len(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding of text is important for the retrivial process of RAG. We embed all our documents and then embed our question and use the embeddings help to perform [semantic search](https://www.elastic.co/what-is/semantic-search) which will improve the results of our search. I''ll touch on this a little more towards the end of this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ingesting Speeches Into A Pinecone Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load all of President Kennedys speeches using a [GCSDirectoryLoader](https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/) which loads an entire directoy in a bucket instead of just a single file. I can see the speeches of his presidency by getting the bucket and loading all the names of the speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JFK had 22 speeches in his presidency.\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client(project=credentials.project_id,\n",
    "                        credentials=credentials)\n",
    "\n",
    "bucket = client.get_bucket(\"prezkennedyspeches\")\n",
    "\n",
    "speeches = [blob.name for blob in bucket.list_blobs()]\n",
    "print(f\"JFK had {len(speeches)} speeches in his presidency.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speeches are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['american-newspaper-publishers-association-19610427.json',\n",
       " 'american-society-of-newspaper-editors-19610420.json',\n",
       " 'american-university-19630610.json',\n",
       " 'americas-cup-dinner-19620914.json',\n",
       " 'berlin-crisis-19610725.json',\n",
       " 'berlin-w-germany-rudolph-wilde-platz-19630626.json',\n",
       " 'civil-rights-radio-and-television-report-19630611.json',\n",
       " 'cuba-radio-and-television-report-19621022.json',\n",
       " 'inaugural-address-19610120.json',\n",
       " 'inaugural-anniversary-19620120.json',\n",
       " 'irish-parliament-19630628.json',\n",
       " 'latin-american-diplomats-washington-dc-19610313.json',\n",
       " 'massachusetts-general-court-19610109.json',\n",
       " 'peace-corps-establishment-19610301.json',\n",
       " 'philadelphia-pa-19620704.json',\n",
       " 'rice-university-19620912.json',\n",
       " 'united-nations-19610925.json',\n",
       " 'united-states-congress-special-message-19610525.json',\n",
       " 'university-of-california-berkeley-19620323.json',\n",
       " 'university-of-mississippi-19620930.json',\n",
       " 'vanderbilt-university-19630518.json',\n",
       " 'yale-university-19620611.json']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I load all of the speeches using the [GCSDirectoryLoader](https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/) and split them into chunks of size 2,000 characters with 100 characters overlapping using the`load_and_split` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GCSDirectoryLoader(\n",
    "                project_name=credentials.project_id,\n",
    "                bucket=\"prezkennedyspeches\",\n",
    "                loader_func=load_json\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "\n",
    "documents = loader.load_and_split(text_splitter)\n",
    "print(f\"There are {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to connect to Pinecone and ingest the data into the vector database. I can create the connection to Pinecone using the command,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create an index in Pinecone to store the documents. An index is basically a collection of embedded documents, similar to a table in a traditional database. [Vector databases](https://en.wikipedia.org/wiki/Vector_database) are specialized databases that allow for storage of vectors as well as for fast searches and retrivials. The vectors have numerical values and represents the documents in embedded form. The vectors are usually high dimensional (in our case 1,536 dimensions) and dense. However, compared to [other representations of text](http://michael-harmon.com/blog/NLP1.html) such as the [Bag-Of-Words model](https://en.wikipedia.org/wiki/Bag-of-words_model) embedding vectors are relatively low dimensional. There are many benefits of vector embeddings and one of the most important is the ability to measure [semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity#:~:text=Semantic%20similarity%20is%20a%20metric,as%20opposed%20to%20lexicographical%20similarity.) between two vectors. This allows us to measures the degree of similarity between pieces of text based on their meaning, rather than just the words used like would be the case with the Bag-Of-Words model. This property of embeddings is depicted below in the classic example,\n",
    "\n",
    "<p align=\"center\">\n",
    "<figure>\n",
    "<img src=\"images/embedding.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<figcaption>\n",
    "Source: https://medium.com/@hari4om/word-embedding-d816f643140\n",
    "</figcaption>\n",
    "</figure>\n",
    "</p>\n",
    "\n",
    "Words that have similar \"meaning\" and or are used in the same context like \"cat\" and \"kitten\" are closer together when represented as vectors in the embedding space then they are to the word \"house\". Embeddings allows to allow capture intrinsic relationships between words, such as the fact that \"man\" is to \"king\" as \"woman\" is to \"queen\". \n",
    "\n",
    "The ability to capture and measure the closeness of words and text using embeddings allows us to perform semantic search. Semantic search will be extremely important for RAG models and will be discussed more in the next post. For now I'll give the index a name and declare the dimension of the vectors it will hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"prez-speeches\"\n",
    "dim = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I delete the index if it exists to clear it of all prior records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the index if it exists\n",
    "if pc.has_index(index_name):\n",
    "    pc.delete_index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll create the index that contains vectors of size `dim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the index\n",
    "pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "                  cloud=\"aws\",\n",
    "                  region=\"us-east-1\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have to declare a metric that is useful for the search. I can list the available indexes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"name\": \"prez-speeches\",\n",
       "        \"dimension\": 1536,\n",
       "        \"metric\": \"cosine\",\n",
       "        \"host\": \"prez-speeches-2307pwa.svc.aped-4627-b74a.pinecone.io\",\n",
       "        \"spec\": {\n",
       "            \"serverless\": {\n",
       "                \"cloud\": \"aws\",\n",
       "                \"region\": \"us-east-1\"\n",
       "            }\n",
       "        },\n",
       "        \"status\": {\n",
       "            \"ready\": true,\n",
       "            \"state\": \"Ready\"\n",
       "        },\n",
       "        \"deletion_protection\": \"disabled\"\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get the statistics on the index we created, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows us that we can hold vectors of size 1,536 dimensions and that we have a total of 0 vectors currently in the index. \n",
    "\n",
    "To ingest documents into the database as vectors we instantiate the [PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html) object, connect it to the index and pass the embedding object,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = PineconeVectorStore(\n",
    "                    pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "                    embedding=embedding,\n",
    "                    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll load the documents into the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = vectordb.from_documents(\n",
    "                            documents=documents, \n",
    "                            embedding=embedding, \n",
    "                            index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood LangChain will call the [embedding.embed_documents](https://python.langchain.com/docs/integrations/text_embedding/openai/#embed-multiple-texts) method to convert the documents from text to numerical vectors and then ingest them into the database.\n",
    "\n",
    "One of the beautiful things about LangChain is how the consistency of the API allows for easily swapping out and replacing different components of LLM applications. For instance one can switch to using a [Chroma](https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html#langchain_chroma.vectorstores.Chroma) database and the syntax remains exactly the same! This characterstic of LangChain is important as each of the underlying databases and embedding models has their own API methods that are not necssarily consistent. Howevever, using LangChain we do have a consistent API and do not need to learn the different syntax for the different backends.\n",
    "\n",
    "Now let's get the stats on the index again,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 180}},\n",
      " 'total_vector_count': 180}\n"
     ]
    }
   ],
   "source": [
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are vectors ingested! \n",
    "\n",
    "Now I can get the Pinecone API directl to get the index to use it to perform semantic search,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to perform search for the semanticly closest documents to the queries. For instance I'll use the query,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Kennedy feel about the Berlin Wall?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I can perform search on the vector database I need to embed this text into a numerical vector,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = embedding.embed_query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can find the 5 closest vectors to the query in the database,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '64fc63a1-79fd-4b40-bf8c-09f0617b9f0f',\n",
       "              'score': 0.857092857,\n",
       "              'values': []},\n",
       "             {'id': '0fa5431f-a374-429e-a622-a1ed1c2b0a21',\n",
       "              'score': 0.851538301,\n",
       "              'values': []},\n",
       "             {'id': '121366d4-9f46-4f52-8e56-2523bf1c9c8f',\n",
       "              'score': 0.848420858,\n",
       "              'values': []},\n",
       "             {'id': '99fb84ef-f4b8-4503-9c76-2ae748703c44',\n",
       "              'score': 0.829222679,\n",
       "              'values': []},\n",
       "             {'id': '55e41b5a-c209-4f18-826e-baac4653f085',\n",
       "              'score': 0.828864217,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 5}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = index.query(vector=query, top_k=5)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results contain the similarity score as well as the document `id`. I can get the most relevant document by getting the first `id` in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = matches[\"matches\"][0].get('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can get the document for that `id` with the `fetch` method of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'berlin-crisis-19610725',\n",
       " 'seq_num': 1.0,\n",
       " 'source': 'gs://prezkennedyspeches/berlin-crisis-19610725.json',\n",
       " 'text': 'Listen to the speech. \\xa0\\xa0 View related documents. \\nPresident John F. Kennedy\\nThe White House\\nJuly 25, 1961\\nGood evening:\\nSeven weeks ago tonight I returned from Europe to report on my meeting with Premier Khrushchev and the others. His grim warnings about the future of the world, his aide memoire on Berlin, his subsequent speeches and threats which he and his agents have launched, and the increase in the Soviet military budget that he has announced, have all prompted a series of decisions by the Administration and a series of consultations with the members of the NATO organization. In Berlin, as you recall, he intends to bring to an end, through a stroke of the pen, first our legal rights to be in West Berlin --and secondly our ability to make good on our commitment to the two million free people of that city. That we cannot permit.\\nWe are clear about what must be done--and we intend to do it. I want to talk frankly with you tonight about the first steps that we shall take. These actions will require sacrifice on the part of many of our citizens. More will be required in the future. They will require, from all of us, courage and perseverance in the years to come. But if we and our allies act out of strength and unity of purpose--with calm determination and steady nerves--using restraint in our words as well as our weapons--I am hopeful that both peace and freedom will be sustained.\\nThe immediate threat to free men is in West Berlin. But that isolated outpost is not an isolated problem. The threat is worldwide. Our effort must be equally wide and strong, and not be obsessed by any single manufactured crisis. We face a challenge in Berlin, but there is also a challenge in Southeast Asia, where the borders are less guarded, the enemy harder to find, and the dangers of communism less apparent to those who have so little. We face a challenge in our own hemisphere, and indeed wherever else the freedom of human beings is at stake.',\n",
       " 'title': 'Radio and Television Report to the American People on the Berlin Crisis, July 25, 1961',\n",
       " 'url': 'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/berlin-crisis-19610725'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = index.fetch([id])\n",
    "result['vectors'][id][\"metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can repeat the same exercise using the LangChain [PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html) api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.search(query=question, search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'berlin-crisis-19610725',\n",
       " 'seq_num': 1.0,\n",
       " 'source': 'gs://prezkennedyspeches/berlin-crisis-19610725.json',\n",
       " 'title': 'Radio and Television Report to the American People on the Berlin Crisis, July 25, 1961',\n",
       " 'url': 'https://www.jfklibrary.org//archives/other-resources/john-f-kennedy-speeches/berlin-crisis-19610725'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same which is to be expected!\n",
    "\n",
    "### 5. Next Steps\n",
    "In this post I covered how to scape websites using the [aysncio](https://docs.python.org/3/library/asyncio.html) and write them to [Google Cloud Storage](https://cloud.google.com/storage?hl=en). After that we covered how to use [LangChain](https://www.langchain.com/) to load text from cloud storage, chunk and embedded it using [OpenAI](https://openai.com/]) Embeddings. Then we coved how to store the embedded documents as vectors in a Pinecone vector database and perform semantic search. In the next blog post I will build off using semantic search with Pinecone to build and deploy a RAG application that can answer questions on President Kennedy's speeches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
